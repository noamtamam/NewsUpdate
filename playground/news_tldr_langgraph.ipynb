{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News TL;DR using Langgraph (Too Long Didn't Read)\n",
    "\n",
    "## Overview\n",
    "This project demonstrates the creation of a news summarization agent uses large language models (LLMs) for decision making and summarization as well as a news API calls. The integration of LangGraph to coordinate sequential and cyclical processes, open-ai to choose and condense articles, newsAPI to retrieve relevant article metadata, and BeautifulSoup for web scraping allows for the generation of relevant current event article TL;DRs from a single query.\n",
    "\n",
    "## Motivation\n",
    "Although LLMs demonstrate excellent conversational and educational ability, they lack access to knowledge of current events. This project allow users to ask about a news topic they are interested and receive a TL;DR of relevant articles. The goal is to allow users to conveniently follow their interest and stay current with their connection to world events.\n",
    "\n",
    "## Key Components\n",
    "1. **LangGraph**: Orchestrates the overall workflow, managing the flow of data between different stages of the process.\n",
    "2. **GPT-4o-mini (via LangChain)**: Generates search terms, selects relevant articles, parses html, provides article summaries\n",
    "3. **NewsAPI**: Retrieves article metadata from keyword search\n",
    "4. **BeautifulSoup**: Retrieves html from page\n",
    "5. **Asyncio**: Allows separate LLM calls to be made concurrently for speed efficiency.\n",
    "\n",
    "## Method\n",
    "The news research follows these high-level steps:\n",
    "\n",
    "1. **NewsAPI Parameter Creation (LLM 1)**: Given a user query, the model generates a formatted parameter dict for the news search.\n",
    "\n",
    "2. **Article Metadata Retrieval**: An API call to NewsAPI retrieves relevant article metadata.\n",
    "\n",
    "3. **Article Text Retrieval**: Beautiful Soup scrapes the full article text from the urls to ensure validity.\n",
    "\n",
    "4. **Conditional Logic**: Conditional logic either: repeats 1-3 if article threshold not reached, proceeds to step 5, end with no articles found.\n",
    "\n",
    "5. **Relevant Article Selection (LLM 2)**: The model selects urls from the most relevant n-articles for the user query based on the short synopsis provided by the API.\n",
    "\n",
    "6. **Generate TL;DR (LLM 3+)**: A summarized set of bullet points for each article is generated concurrently with Asyncio.\n",
    "\n",
    "This workflow is managed by LangGraph to make sure that the appropriate prompt is fed to the each LLM call.\n",
    "\n",
    "## Conclusion\n",
    "This news TL;DR agent highlights the utility of coordinating successive LLM generations in order to\n",
    "achieve a higher level goal.\n",
    "\n",
    "Although the current implementation only retrieves bulleted summaries, it could be elaborated to start\n",
    "a dialogue with the user that could allow them to ask questions about the article and get \n",
    "more information or to collectively generate a coherent opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langgraph -q\n",
    "# !pip install langchain-openai -q\n",
    "# !pip install langchain-core -q\n",
    "# !pip install pydantic -q\n",
    "# !pip install python-dotenv -q\n",
    "# !pip install newsapi-python -q\n",
    "# !pip install beautifulsoup4 -q\n",
    "# !pip install ipython -q\n",
    "# !pip install nest_asyncio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import Graph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from newsapi import NewsApiClient\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from IPython.display import display, Image as IPImage\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get an NewsAPI Key\n",
    "* create a free developer account at https://newsapi.org/\n",
    "* 100 requests per day\n",
    "* articles between 1 day and 1 month old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LLM Model\n",
    "* create an account and register a credit card at https://platform.openai.com/chat-completions\n",
    "* create an API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Your Environmental Variables (Optional)\n",
    "Create a file named `.env` in the same directory as this notebook with the following\n",
    "```\n",
    "OPENAI_API_KEY = 'your-api-key'\n",
    "NEWSAPI_KEY = 'your-api-key'\n",
    "```\n",
    "\n",
    "If you skip this step, you will be asked to input all API keys once each time you start this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model and Environmental Variables\n",
    "\n",
    "If you're not running a local model with Ollama, the next cell will ask for your OPENAI_API_KEY and\n",
    "securely add it as an environmental variable. It will not persist in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "llm = ChatOpenAI(model=model,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEWSAPI_KEY  and OPEN successfully loaded from .env.\n"
     ]
    }
   ],
   "source": [
    "newsapi_key = os.getenv(\"NEWSAPI_KEY\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if newsapi_key and openai_key:\n",
    "        print(\"NEWSAPI_KEY  and OPEN successfully loaded from .env.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters Earth's atmosphere, it collides with air molecules and small particles. Sunlight is made up of different colors, each with a different wavelength, and blue light has a shorter wavelength than most other colors.\\n\\nBecause blue light is scattered in all directions by the gases and particles in the atmosphere, it becomes the dominant color that we see when we look up at the sky. During sunrise and sunset, the sunlight has to travel through a longer path in the atmosphere, scattering away the shorter blue wavelengths and allowing the longer red and orange wavelengths to dominate, which is why we often see beautiful red and orange hues at those times.\\n\\nIn summary, the blue color of the sky results from the scattering of sunlight by atmospheric particles, with shorter wavelengths (blue) being scattered more than longer wavelengths.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Why is the sky blue?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': {'id': 'techcrunch', 'name': 'TechCrunch'},\n",
       " 'author': 'Ivan Mehta',\n",
       " 'title': 'Opera adds its Aria AI assistant to Opera Mini browser',\n",
       " 'description': 'Norway-based browser company Opera said today that it is making its AI assistant Aria available to Opera Mini users on Android.\\xa0This move will help users with low-end devices and data constraints access better AI features. Aria AI can help users get the lates…',\n",
       " 'url': 'https://techcrunch.com/2025/04/16/opera-adds-its-aria-ai-assistant-to-opera-mini-browser/',\n",
       " 'urlToImage': 'https://techcrunch.com/wp-content/uploads/2025/04/Opera-Mini-Aria.png?resize=1200,675',\n",
       " 'publishedAt': '2025-04-16T08:00:00Z',\n",
       " 'content': 'Norway-based browser company Opera said today that it is making its AI assistant Aria available to Opera Mini users on Android.\\xa0This move will help users with low-end devices and data constraints acc… [+1555 chars]'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsapi = NewsApiClient(api_key=os.getenv('NEWSAPI_KEY'))\n",
    "\n",
    "query = 'ai news of the day'\n",
    "\n",
    "all_articles = newsapi.get_everything(q=query,\n",
    "                                      sources='google-news,bbc-news,techcrunch',\n",
    "                                      domains='techcrunch.com, bbc.co.uk',\n",
    "                                      language='en',\n",
    "                                      sort_by='relevancy',)\n",
    "\n",
    "\n",
    "all_articles['articles'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Structures\n",
    "\n",
    "Define the GraphState class. Each user query will be added to a new instance of this class, which will be passed\n",
    "through the LangGraph structure while collect outputs from each step. When it reaches the END node, it's final\n",
    "result will be returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    news_query: Annotated[str, \"Input query to extract news search parameters from.\"]\n",
    "    num_searches_remaining: Annotated[int, \"Number of articles to search for.\"]\n",
    "    newsapi_params: Annotated[dict, \"Structured argument for the News API.\"]\n",
    "    past_searches: Annotated[List[dict], \"List of search params already used.\"]\n",
    "    articles_metadata: Annotated[list[dict], \"Article metadata response from the News API\"]\n",
    "    scraped_urls: Annotated[List[str], \"List of urls already scraped.\"]\n",
    "    num_articles_tldr: Annotated[int, \"Number of articles to create TL;DR for.\"]\n",
    "    potential_articles: Annotated[List[dict[str, str, str]], \"Article with full text to consider summarizing.\"]\n",
    "    tldr_articles: Annotated[List[dict[str, str, str]], \"Selected article TL;DRs.\"]\n",
    "    formatted_results: Annotated[str, \"Formatted results to display.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define NewsAPI argument data structure with Pydantic\n",
    "* the model will create a formatted dictionary of params for the NewsAPI call\n",
    "* the NewsApiParams class inherits from the Pydantic BaseModel\n",
    "* Langchain will parse and feed paramd descriptions to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsApiParams(BaseModel):\n",
    "    q: str = Field(description=\"1-3 concise keyword search terms that are not too specific\")\n",
    "    sources: str =Field(description=\"comma-separated list of sources from: 'abc-news,abc-news-au,associated-press,australian-financial-review,axios,bbc-news,bbc-sport,bloomberg,business-insider,cbc-news,cbs-news,cnn,financial-post,fortune'\")\n",
    "    from_param: str = Field(description=\"date in format 'YYYY-MM-DD' Two days ago minimum. Extend up to 30 days on second and subsequent requests.\")\n",
    "    to: str = Field(description=\"date in format 'YYYY-MM-DD' today's date unless specified\")\n",
    "    language: str = Field(description=\"language of articles 'en' unless specified one of ['ar', 'de', 'en', 'es', 'fr', 'he', 'it', 'nl', 'no', 'pt', 'ru', 'se', 'ud', 'zh']\")\n",
    "    sort_by: str = Field(description=\"sort by 'relevancy', 'popularity', or 'publishedAt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Graph Functions\n",
    "\n",
    "Define the functions (nodes) that will be used in the LangGraph workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_newsapi_params(state: GraphState) -> GraphState:\n",
    "    \"\"\"Based on the query, generate News API params.\"\"\"\n",
    "    # initialize parser to define the structure of the response\n",
    "    parser = JsonOutputParser(pydantic_object=NewsApiParams)\n",
    "\n",
    "    # retrieve today's date\n",
    "    today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # retrieve list of past search params\n",
    "    past_searches = state[\"past_searches\"]\n",
    "\n",
    "    # retrieve number of searches remaining\n",
    "    num_searches_remaining = state[\"num_searches_remaining\"]\n",
    "\n",
    "    # retrieve the user's query\n",
    "    news_query = state[\"news_query\"]\n",
    "\n",
    "    template = \"\"\"\n",
    "    Today is {today_date}.\n",
    "\n",
    "    Create a param dict for the News API based on the user query:\n",
    "    {query}\n",
    "\n",
    "    These searches have already been made. Loosen the search terms to get more results.\n",
    "    {past_searches}\n",
    "    \n",
    "    Following these formatting instructions:\n",
    "    {format_instructions}\n",
    "\n",
    "    Including this one, you have {num_searches_remaining} searches remaining.\n",
    "    If this is your last search, use all news sources and a 30 days search range.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a prompt template to merge the query, today's date, and the format instructions\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=template,\n",
    "        variables={\"today\": today_date, \"query\": news_query, \"past_searches\": past_searches, \"num_searches_remaining\": num_searches_remaining},\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "    # create prompt chain template\n",
    "    chain = prompt_template | llm | parser\n",
    "\n",
    "    # invoke the chain with the news api query\n",
    "    result = chain.invoke({\"query\": news_query, \"today_date\": today_date, \"past_searches\": past_searches, \"num_searches_remaining\": num_searches_remaining})\n",
    "\n",
    "    # update the state\n",
    "    state[\"newsapi_params\"] = result\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_articles_metadata(state: GraphState) -> GraphState:\n",
    "    \"\"\"Using the NewsAPI params, perform api call.\"\"\"\n",
    "    # parameters generated for the News API\n",
    "    newsapi_params = state[\"newsapi_params\"]\n",
    "\n",
    "    # decrement the number of searches remaining\n",
    "    state['num_searches_remaining'] -= 1\n",
    "\n",
    "    try:\n",
    "        # create a NewsApiClient object\n",
    "        newsapi = NewsApiClient(api_key=os.getenv('NEWSAPI_KEY'))\n",
    "        \n",
    "        # retreive the metadata of the new articles\n",
    "        articles = newsapi.get_everything(**newsapi_params)\n",
    "\n",
    "        # append this search term to the past searches to avoid duplicates\n",
    "        state['past_searches'].append(newsapi_params)\n",
    "\n",
    "        # load urls that have already been returned and scraped\n",
    "        scraped_urls = state[\"scraped_urls\"]\n",
    "\n",
    "        # filter out articles that have already been scraped\n",
    "        new_articles = []\n",
    "        for article in articles['articles']:\n",
    "            if article['url'] not in scraped_urls and len(state['potential_articles']) + len(new_articles) < 10:\n",
    "                new_articles.append(article)\n",
    "\n",
    "        # reassign new articles to the state\n",
    "        state[\"articles_metadata\"] = new_articles\n",
    "\n",
    "    # handle exceptions\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_articles_text(state: GraphState) -> GraphState:\n",
    "    \"\"\"Web scrapes to retrieve article text.\"\"\"\n",
    "    # load retrieved article metadata\n",
    "    articles_metadata = state[\"articles_metadata\"]\n",
    "    # Add headers to simulate a browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # create list to store valid article dicts\n",
    "    potential_articles = []\n",
    "\n",
    "    # iterate over the urls\n",
    "    for article in articles_metadata:\n",
    "        # extract the url\n",
    "        url = article['url']\n",
    "\n",
    "        # use beautiful soup to extract the article content\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # parse the HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # find the article content\n",
    "            text = soup.get_text(strip=True)\n",
    "\n",
    "            # append article dict to list\n",
    "            potential_articles.append({\"title\": article[\"title\"], \"url\": url, \"description\": article[\"description\"], \"text\": text})\n",
    "\n",
    "            # append the url to the processed urls\n",
    "            state[\"scraped_urls\"].append(url)\n",
    "\n",
    "    # append the processed articles to the state\n",
    "    state[\"potential_articles\"].extend(potential_articles)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_urls(state: GraphState) -> GraphState:\n",
    "    \"\"\"Based on the article synoses, choose the top-n articles to summarize.\"\"\"\n",
    "    news_query = state[\"news_query\"]\n",
    "    num_articles_tldr = state[\"num_articles_tldr\"]\n",
    "    \n",
    "    # load all processed articles with full text but no summaries\n",
    "    potential_articles = state[\"potential_articles\"]\n",
    "\n",
    "    # format the metadata\n",
    "    formatted_metadata = \"\\n\".join([f\"{article['url']}\\n{article['description']}\\n\" for article in potential_articles])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Based on the user news query:\n",
    "    {news_query}\n",
    "\n",
    "    Reply with a list of strings of up to {num_articles_tldr} relevant urls.\n",
    "    Don't add any urls that are not relevant or aren't listed specifically.\n",
    "    {formatted_metadata}\n",
    "    \"\"\"\n",
    "    result = llm.invoke(prompt).content\n",
    "\n",
    "    # use regex to extract the urls as a list\n",
    "    url_pattern = r'(https?://[^\\s\",]+)'\n",
    "\n",
    "    # Find all URLs in the text\n",
    "    urls = re.findall(url_pattern, result)\n",
    "\n",
    "    # add the selected article metadata to the state\n",
    "    tldr_articles = [article for article in potential_articles if article['url'] in urls]\n",
    "\n",
    "    # tldr_articles = [article for article in potential_articles if article['url'] in urls]\n",
    "    state[\"tldr_articles\"] = tldr_articles\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def summarize_articles_parallel(state: GraphState) -> GraphState:\n",
    "    \"\"\"Summarize the articles based on full text.\"\"\"\n",
    "    tldr_articles = state[\"tldr_articles\"]\n",
    "\n",
    "    # prompt = \"\"\"\n",
    "    # Summarize the article text in a bulleted tl;dr. Each line should start with a hyphen -\n",
    "    # {article_text}\n",
    "    # \"\"\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    Create a * bulleted summarizing tldr for the article:\n",
    "    {text}\n",
    "      \n",
    "    Be sure to follow the following format exaxtly with nothing else:\n",
    "    {title}\n",
    "    {url}\n",
    "    * tl;dr bulleted summary\n",
    "    * use bullet points for each sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # iterate over the selected articles and collect summaries synchronously\n",
    "    for i in range(len(tldr_articles)):\n",
    "        text = tldr_articles[i][\"text\"]\n",
    "        title = tldr_articles[i][\"title\"]\n",
    "        url = tldr_articles[i][\"url\"]\n",
    "        # invoke the llm synchronously\n",
    "        result = llm.invoke(prompt.format(title=title, url=url, text=text))\n",
    "        tldr_articles[i][\"summary\"] = result.content\n",
    "\n",
    "    state[\"tldr_articles\"] = tldr_articles\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results(state: GraphState) -> GraphState:\n",
    "    \"\"\"Format the results for display.\"\"\"\n",
    "    # load a list of past search queries\n",
    "    q = [newsapi_params[\"q\"] for newsapi_params in state[\"past_searches\"]]\n",
    "    formatted_results = f\"Here are the top {len(state['tldr_articles'])} articles based on search terms:\\n{', '.join(q)}\\n\\n\"\n",
    "\n",
    "    # load the summarized articles\n",
    "    tldr_articles = state[\"tldr_articles\"]\n",
    "\n",
    "    # format article tl;dr summaries\n",
    "    tldr_articles = \"\\n\\n\".join([f\"{article['summary']}\" for article in tldr_articles])\n",
    "\n",
    "    # concatenate summaries to the formatted results\n",
    "    formatted_results += tldr_articles\n",
    "\n",
    "    state[\"formatted_results\"] = formatted_results\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up LangGraph Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up decision logic to try to retrieve `num_searches_remaining` articles, while limiting attempts to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def articles_text_decision(state: GraphState) -> str:\n",
    "    \"\"\"Check results of retrieve_articles_text to determine next step.\"\"\"\n",
    "    if state[\"num_searches_remaining\"] == 0:\n",
    "        # if no articles with text were found return END\n",
    "        if len(state[\"potential_articles\"]) == 0:\n",
    "            state[\"formatted_results\"] = \"No articles with text found.\"\n",
    "            return \"END\"\n",
    "        # if some articles were found, move on to selecting the top urls\n",
    "        else:\n",
    "            return \"select_top_urls\"\n",
    "    else:\n",
    "        # if the number of articles found is less than the number of articles to summarize, continue searching\n",
    "        if len(state[\"potential_articles\"]) < state[\"num_articles_tldr\"]:\n",
    "            return \"generate_newsapi_params\"\n",
    "        # otherwise move on to selecting the top urls\n",
    "        else:\n",
    "            return \"select_top_urls\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the LangGraph workflow by adding nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = Graph()\n",
    "\n",
    "workflow.set_entry_point(\"generate_newsapi_params\")\n",
    "\n",
    "workflow.add_node(\"generate_newsapi_params\", generate_newsapi_params)\n",
    "workflow.add_node(\"retrieve_articles_metadata\", retrieve_articles_metadata)\n",
    "workflow.add_node(\"retrieve_articles_text\", retrieve_articles_text)\n",
    "workflow.add_node(\"select_top_urls\", select_top_urls)\n",
    "workflow.add_node(\"summarize_articles_parallel\", summarize_articles_parallel)\n",
    "workflow.add_node(\"format_results\", format_results)\n",
    "# workflow.add_node(\"add_commentary\", add_commentary)\n",
    "\n",
    "workflow.add_edge(\"generate_newsapi_params\", \"retrieve_articles_metadata\")\n",
    "workflow.add_edge(\"retrieve_articles_metadata\", \"retrieve_articles_text\")\n",
    "# # if the number of articles with parseable text is less than number requested, then search for more articles\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve_articles_text\",\n",
    "    articles_text_decision,\n",
    "    {\n",
    "        \"generate_newsapi_params\": \"generate_newsapi_params\",\n",
    "        \"select_top_urls\": \"select_top_urls\",\n",
    "        \"END\": END\n",
    "    }\n",
    "    )\n",
    "workflow.add_edge(\"select_top_urls\", \"summarize_articles_parallel\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"summarize_articles_parallel\",\n",
    "    lambda state: \"format_results\" if len(state[\"tldr_articles\"]) > 0 else \"END\",\n",
    "    {\n",
    "        \"format_results\": \"format_results\",\n",
    "        \"END\": END\n",
    "    }\n",
    "    )\n",
    "workflow.add_edge(\"format_results\", END)\n",
    "\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Graph Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Starting Chromium download.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Chromium downloadable not found at https://storage.googleapis.com/chromium-browser-snapshots/Win_x64/1181205/chrome-win.zip: Received <?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Details>No such object: chromium-browser-snapshots/Win_x64/1181205/chrome-win.zip</Details></Error>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m display(\n\u001b[32m      2\u001b[39m     IPImage(\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMermaidDrawMethod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPYPPETEER\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     )\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# View\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# from IPython.display import Image, display\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# display(Image(app.get_graph().draw_mermaid_png(max_retries=5, retry_delay=2.0, draw_method=MermaidDrawMethod.PYPPETEER),  ))\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# await show_mermaid_graph()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Documents\\projects\\LLM\\NewsUpdate\\venv-news\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:685\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    679\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    680\u001b[39m     curve_style=curve_style,\n\u001b[32m    681\u001b[39m     node_colors=node_colors,\n\u001b[32m    682\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    683\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    684\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Documents\\projects\\LLM\\NewsUpdate\\venv-news\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:287\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m draw_method == MermaidDrawMethod.PYPPETEER:\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01masyncio\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     img_bytes = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_render_mermaid_using_pyppeteer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m    293\u001b[39m     img_bytes = _render_mermaid_using_api(\n\u001b[32m    294\u001b[39m         mermaid_syntax,\n\u001b[32m    295\u001b[39m         output_file_path=output_file_path,\n\u001b[32m   (...)\u001b[39m\u001b[32m    298\u001b[39m         retry_delay=retry_delay,\n\u001b[32m    299\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Documents\\projects\\LLM\\NewsUpdate\\venv-news\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Documents\\projects\\LLM\\NewsUpdate\\venv-news\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:267\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    265\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    266\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    269\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Documents\\projects\\LLM\\NewsUpdate\\venv-news\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:325\u001b[39m, in \u001b[36m_render_mermaid_using_pyppeteer\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, padding, device_scale_factor)\u001b[39m\n\u001b[32m    322\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInstall Pyppeteer to use the Pyppeteer method: `pip install pyppeteer`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m browser = \u001b[38;5;28;01mawait\u001b[39;00m launch()\n\u001b[32m    326\u001b[39m page = \u001b[38;5;28;01mawait\u001b[39;00m browser.newPage()\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# Setup Mermaid JS\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Documents\\projects\\LLM\\NewsUpdate\\venv-news\\Lib\\site-packages\\pyppeteer\\launcher.py:307\u001b[39m, in \u001b[36mlaunch\u001b[39m\u001b[34m(options, **kwargs)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlaunch\u001b[39m(options: \u001b[38;5;28mdict\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any) -> Browser:\n\u001b[32m    240\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Start chrome process and return :class:`~pyppeteer.browser.Browser`.\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[33;03m    This function is a shortcut to :meth:`Launcher(options, **kwargs).launch`.\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[33;03m    Available options are:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    305\u001b[39m \u001b[33;03m        option with extreme caution.\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mLauncher\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.launch()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Documents\\projects\\LLM\\NewsUpdate\\venv-news\\Lib\\site-packages\\pyppeteer\\launcher.py:120\u001b[39m, in \u001b[36mLauncher.__init__\u001b[39m\u001b[34m(self, options, **kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chromeExecutable:\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_chromium():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[43mdownload_chromium\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m.chromeExecutable = \u001b[38;5;28mstr\u001b[39m(chromium_executable())\n\u001b[32m    123\u001b[39m \u001b[38;5;28mself\u001b[39m.cmd = [\u001b[38;5;28mself\u001b[39m.chromeExecutable] + \u001b[38;5;28mself\u001b[39m.chromeArguments\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Documents\\projects\\LLM\\NewsUpdate\\venv-news\\Lib\\site-packages\\pyppeteer\\chromium_downloader.py:138\u001b[39m, in \u001b[36mdownload_chromium\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload_chromium\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    137\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Download and extract chromium.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     extract_zip(\u001b[43mdownload_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, DOWNLOADS_FOLDER / REVISION)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Documents\\projects\\LLM\\NewsUpdate\\venv-news\\Lib\\site-packages\\pyppeteer\\chromium_downloader.py:82\u001b[39m, in \u001b[36mdownload_zip\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m     80\u001b[39m r = http.request(\u001b[33m'\u001b[39m\u001b[33mGET\u001b[39m\u001b[33m'\u001b[39m, url, preload_content=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r.status >= \u001b[32m400\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mChromium downloadable not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mReceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr.data.decode()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# 10 * 1024\u001b[39;00m\n\u001b[32m     85\u001b[39m _data = BytesIO()\n",
      "\u001b[31mOSError\u001b[39m: Chromium downloadable not found at https://storage.googleapis.com/chromium-browser-snapshots/Win_x64/1181205/chrome-win.zip: Received <?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Details>No such object: chromium-browser-snapshots/Win_x64/1181205/chrome-win.zip</Details></Error>.\n"
     ]
    }
   ],
   "source": [
    "# display(\n",
    "#     IPImage(\n",
    "#         app.get_graph().draw_mermaid_png(\n",
    "#             draw_method=MermaidDrawMethod.PYPPETEER\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "# View\n",
    "# from IPython.display import Image, display\n",
    "# display(Image(app.get_graph().draw_mermaid_png(max_retries=5, retry_delay=2.0, draw_method=MermaidDrawMethod.PYPPETEER),  ))\n",
    "\n",
    "# from langchain_core.runnables.graph_mermaid import MermaidDrawMethod\n",
    "\n",
    "# async def show_mermaid_graph():\n",
    "#     png_data = await app.get_graph().draw_mermaid_png(max_retries=5, retry_delay=2.0)\n",
    "#     display(Image(png_data))\n",
    "\n",
    "# await show_mermaid_graph()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Workflow Function\n",
    "\n",
    "Define a function to run the workflow and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_workflow(query: str, num_searches_remaining: int = 10, num_articles_tldr: int = 3):\n",
    "    \"\"\"Run the LangGraph workflow and display results.\"\"\"\n",
    "    initial_state = {\n",
    "        \"news_query\": query,\n",
    "        \"num_searches_remaining\": num_searches_remaining,\n",
    "        \"newsapi_params\": {},\n",
    "        \"past_searches\": [],\n",
    "        \"articles_metadata\": [],\n",
    "        \"scraped_urls\": [],\n",
    "        \"num_articles_tldr\": num_articles_tldr,\n",
    "        \"potential_articles\": [],\n",
    "        \"tldr_articles\": [],\n",
    "        \"formatted_results\": \"No articles with text found.\"\n",
    "    }\n",
    "    try:\n",
    "        result = await app.ainvoke(initial_state)\n",
    "        \n",
    "        return result[\"formatted_results\"]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Workflow\n",
    "\n",
    "Run the workflow with a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the top 2 articles based on search terms:\n",
      "Israel news\n",
      "\n",
      "Israeli strike on south Gaza hospital kills senior Hamas official  \n",
      "https://abcnews.go.com/International/israeli-strike-south-gaza-hospital-kills-senior-hamas/story?id=120089158  \n",
      "* An Israeli airstrike on Nasser Hospital in Khan Younis killed senior Hamas official Ismail Barhoum.  \n",
      "* Hamas confirmed Barhoum was receiving medical treatment in the hospital at the time of the attack.  \n",
      "* The Israel Defense Forces (IDF) described Barhoum as a \"key terrorist\" operating inside the hospital.  \n",
      "* The strike specifically targeted the hospital's surgical wing while many patients were present.  \n",
      "* The Hamas-run Gaza Health Ministry reported that many individuals, including medical personnel, were injured in the attack.  \n",
      "* Nasser Hospital has previously been targeted by Israeli forces, and Israel alleges Hamas uses it for military purposes.  \n",
      "* Since the resumption of hostilities last week, at least 673 people have reportedly been killed in Israeli strikes.  \n",
      "* The total death toll in Gaza since the onset of conflict on October 7, 2023, has reached 50,021.\n",
      "\n",
      "Israeli operation in Gaza expanding to seize 'large areas,' defense minister says  \n",
      "https://abcnews.go.com/International/israeli-operation-gaza-expanding-seize-large-areas-defense/story?id=120401894  \n",
      "* Israel's military operation in Gaza is expanding to seize large areas and eliminate terrorist infrastructure.  \n",
      "* Defense Minister Israel Katz stated the goal is to \"crush and cleanse\" enemy forces in the region.  \n",
      "* A large-scale evacuation of the Gazan population from conflict areas is underway.  \n",
      "* Katz urged Gazans to act against Hamas to end the ongoing war.  \n",
      "* Israel resumed its assault in March following the release of 33 hostages by Hamas in exchange for Palestinian prisoners.  \n",
      "* Prime Minister Benjamin Netanyahu indicated strikes were renewed due to Hamas's refusal to negotiate on hostages.  \n",
      "* Israeli officials emphasize the intent to destroy Hamas and maintain control over Gaza post-conflict.  \n",
      "* A new directorate for \"voluntary emigration\" from Gaza has been established, aligning with resettlement suggestions from U.S. leadership.  \n",
      "* The resettlement policy has faced accusations of \"ethnic cleansing\" from various organizations, which Israel denies.  \n",
      "* The ongoing conflict has resulted in significant casualties, with over 50,300 reported dead in Gaza since the conflict resumed.  \n"
     ]
    }
   ],
   "source": [
    "query = \"what are the top news about israel?\"\n",
    "print(await run_workflow(query, num_articles_tldr=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
